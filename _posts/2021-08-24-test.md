```python
import os
import pandas as pd 
import numpy as np
import statsmodels.api as sm
```


```python
# 현재경로 확인
os.getcwd()
```




    '/Users/kangjunseo/OneDrive/바탕 화면/머신러닝과 데이터분석 A-Z 올인원 패키지 Online/Part 05~11) Machine Learning/06. 회귀분석/실습코드'




```python
# 데이터 불러오기
boston = pd.read_csv("./Boston_house.csv")
boston
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AGE</th>
      <th>B</th>
      <th>RM</th>
      <th>CRIM</th>
      <th>DIS</th>
      <th>INDUS</th>
      <th>LSTAT</th>
      <th>NOX</th>
      <th>PTRATIO</th>
      <th>RAD</th>
      <th>ZN</th>
      <th>TAX</th>
      <th>CHAS</th>
      <th>Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>65.2</td>
      <td>396.90</td>
      <td>6.575</td>
      <td>0.00632</td>
      <td>4.0900</td>
      <td>2.31</td>
      <td>4.98</td>
      <td>0.538</td>
      <td>15.3</td>
      <td>1</td>
      <td>18.0</td>
      <td>296</td>
      <td>0</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>78.9</td>
      <td>396.90</td>
      <td>6.421</td>
      <td>0.02731</td>
      <td>4.9671</td>
      <td>7.07</td>
      <td>9.14</td>
      <td>0.469</td>
      <td>17.8</td>
      <td>2</td>
      <td>0.0</td>
      <td>242</td>
      <td>0</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>61.1</td>
      <td>392.83</td>
      <td>7.185</td>
      <td>0.02729</td>
      <td>4.9671</td>
      <td>7.07</td>
      <td>4.03</td>
      <td>0.469</td>
      <td>17.8</td>
      <td>2</td>
      <td>0.0</td>
      <td>242</td>
      <td>0</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>45.8</td>
      <td>394.63</td>
      <td>6.998</td>
      <td>0.03237</td>
      <td>6.0622</td>
      <td>2.18</td>
      <td>2.94</td>
      <td>0.458</td>
      <td>18.7</td>
      <td>3</td>
      <td>0.0</td>
      <td>222</td>
      <td>0</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>54.2</td>
      <td>396.90</td>
      <td>7.147</td>
      <td>0.06905</td>
      <td>6.0622</td>
      <td>2.18</td>
      <td>5.33</td>
      <td>0.458</td>
      <td>18.7</td>
      <td>3</td>
      <td>0.0</td>
      <td>222</td>
      <td>0</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>501</th>
      <td>69.1</td>
      <td>391.99</td>
      <td>6.593</td>
      <td>0.06263</td>
      <td>2.4786</td>
      <td>11.93</td>
      <td>9.67</td>
      <td>0.573</td>
      <td>21.0</td>
      <td>1</td>
      <td>0.0</td>
      <td>273</td>
      <td>0</td>
      <td>22.4</td>
    </tr>
    <tr>
      <th>502</th>
      <td>76.7</td>
      <td>396.90</td>
      <td>6.120</td>
      <td>0.04527</td>
      <td>2.2875</td>
      <td>11.93</td>
      <td>9.08</td>
      <td>0.573</td>
      <td>21.0</td>
      <td>1</td>
      <td>0.0</td>
      <td>273</td>
      <td>0</td>
      <td>20.6</td>
    </tr>
    <tr>
      <th>503</th>
      <td>91.0</td>
      <td>396.90</td>
      <td>6.976</td>
      <td>0.06076</td>
      <td>2.1675</td>
      <td>11.93</td>
      <td>5.64</td>
      <td>0.573</td>
      <td>21.0</td>
      <td>1</td>
      <td>0.0</td>
      <td>273</td>
      <td>0</td>
      <td>23.9</td>
    </tr>
    <tr>
      <th>504</th>
      <td>89.3</td>
      <td>393.45</td>
      <td>6.794</td>
      <td>0.10959</td>
      <td>2.3889</td>
      <td>11.93</td>
      <td>6.48</td>
      <td>0.573</td>
      <td>21.0</td>
      <td>1</td>
      <td>0.0</td>
      <td>273</td>
      <td>0</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>505</th>
      <td>80.8</td>
      <td>396.90</td>
      <td>6.030</td>
      <td>0.04741</td>
      <td>2.5050</td>
      <td>11.93</td>
      <td>7.88</td>
      <td>0.573</td>
      <td>21.0</td>
      <td>1</td>
      <td>0.0</td>
      <td>273</td>
      <td>0</td>
      <td>11.9</td>
    </tr>
  </tbody>
</table>
<p>506 rows × 14 columns</p>
</div>




```python
boston_data = boston.drop(['Target'],axis=1)
# boston_data
```


```python
'''
타겟 데이터
1978 보스턴 주택 가격
506개 타운의 주택 가격 중앙값 (단위 1,000 달러)

특징 데이터
CRIM: 범죄율
INDUS: 비소매상업지역 면적 비율
NOX: 일산화질소 농도
RM: 주택당 방 수
LSTAT: 인구 중 하위 계층 비율
B: 인구 중 흑인 비율
PTRATIO: 학생/교사 비율
ZN: 25,000 평방피트를 초과 거주지역 비율
CHAS: 찰스강의 경계에 위치한 경우는 1, 아니면 0
AGE: 1940년 이전에 건축된 주택의 비율
RAD: 방사형 고속도로까지의 거리
DIS: 직업센터의 거리
TAX: 재산세율'''
```




    '\n타겟 데이터\n1978 보스턴 주택 가격\n506개 타운의 주택 가격 중앙값 (단위 1,000 달러)\n\n특징 데이터\nCRIM: 범죄율\nINDUS: 비소매상업지역 면적 비율\nNOX: 일산화질소 농도\nRM: 주택당 방 수\nLSTAT: 인구 중 하위 계층 비율\nB: 인구 중 흑인 비율\nPTRATIO: 학생/교사 비율\nZN: 25,000 평방피트를 초과 거주지역 비율\nCHAS: 찰스강의 경계에 위치한 경우는 1, 아니면 0\nAGE: 1940년 이전에 건축된 주택의 비율\nRAD: 방사형 고속도로까지의 거리\nDIS: 직업센터의 거리\nTAX: 재산세율'



# crim, rm, lstat을 통한 다중 선형 회귀분석


```python
x_data=boston[['CRIM','RM','LSTAT']] ##변수 여러개
target = boston[['Target']]
x_data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>RM</th>
      <th>LSTAT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>6.575</td>
      <td>4.98</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>6.421</td>
      <td>9.14</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>7.185</td>
      <td>4.03</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>6.998</td>
      <td>2.94</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>7.147</td>
      <td>5.33</td>
    </tr>
  </tbody>
</table>
</div>




```python
x_data1 = sm.add_constant(x_data, has_constant='add')

```


```python
multi_model = sm.OLS(target,x_data1)
fitted_multi_model=multi_model.fit()
```


```python
fitted_multi_model.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>Target</td>      <th>  R-squared:         </th> <td>   0.646</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.644</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   305.2</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 27 Jul 2021</td> <th>  Prob (F-statistic):</th> <td>1.01e-112</td>
</tr>
<tr>
  <th>Time:</th>                 <td>22:39:06</td>     <th>  Log-Likelihood:    </th> <td> -1577.6</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3163.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   502</td>      <th>  BIC:               </th> <td>   3180.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   -2.5623</td> <td>    3.166</td> <td>   -0.809</td> <td> 0.419</td> <td>   -8.783</td> <td>    3.658</td>
</tr>
<tr>
  <th>CRIM</th>  <td>   -0.1029</td> <td>    0.032</td> <td>   -3.215</td> <td> 0.001</td> <td>   -0.166</td> <td>   -0.040</td>
</tr>
<tr>
  <th>RM</th>    <td>    5.2170</td> <td>    0.442</td> <td>   11.802</td> <td> 0.000</td> <td>    4.348</td> <td>    6.085</td>
</tr>
<tr>
  <th>LSTAT</th> <td>   -0.5785</td> <td>    0.048</td> <td>  -12.135</td> <td> 0.000</td> <td>   -0.672</td> <td>   -0.485</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>171.754</td> <th>  Durbin-Watson:     </th> <td>   0.822</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 628.308</td> 
</tr>
<tr>
  <th>Skew:</th>          <td> 1.535</td>  <th>  Prob(JB):          </th> <td>3.67e-137</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 7.514</td>  <th>  Cond. No.          </th> <td>    216.</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.



# crim, rm, lstat, b, tax, age, zn, nox, indus 변수를 통한 다중선형회귀분석


```python
## bostan data에서 원하는 변수만 뽑아오기 
x_data2=boston[["CRIM","RM","LSTAT","B","TAX","AGE","ZN","NOX","INDUS"]]
x_data2.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>RM</th>
      <th>LSTAT</th>
      <th>B</th>
      <th>TAX</th>
      <th>AGE</th>
      <th>ZN</th>
      <th>NOX</th>
      <th>INDUS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>6.575</td>
      <td>4.98</td>
      <td>396.90</td>
      <td>296</td>
      <td>65.2</td>
      <td>18.0</td>
      <td>0.538</td>
      <td>2.31</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>6.421</td>
      <td>9.14</td>
      <td>396.90</td>
      <td>242</td>
      <td>78.9</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.07</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>7.185</td>
      <td>4.03</td>
      <td>392.83</td>
      <td>242</td>
      <td>61.1</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.07</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>6.998</td>
      <td>2.94</td>
      <td>394.63</td>
      <td>222</td>
      <td>45.8</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>2.18</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>7.147</td>
      <td>5.33</td>
      <td>396.90</td>
      <td>222</td>
      <td>54.2</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>2.18</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 상수항추기
x_data2_ =sm.add_constant(x_data2,has_constant="add")
```


```python
# 회귀모델 적합
multi_model2=sm.OLS(target,x_data2_)
fitted_multi_model2 = multi_model2.fit()
```


```python
# 결과 출력
fitted_multi_model2.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>Target</td>      <th>  R-squared:         </th> <td>   0.662</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.656</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   108.1</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 27 Jul 2021</td> <th>  Prob (F-statistic):</th> <td>5.76e-111</td>
</tr>
<tr>
  <th>Time:</th>                 <td>22:45:41</td>     <th>  Log-Likelihood:    </th> <td> -1565.5</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3151.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   496</td>      <th>  BIC:               </th> <td>   3193.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   -7.1088</td> <td>    3.828</td> <td>   -1.857</td> <td> 0.064</td> <td>  -14.631</td> <td>    0.413</td>
</tr>
<tr>
  <th>CRIM</th>  <td>   -0.0453</td> <td>    0.036</td> <td>   -1.269</td> <td> 0.205</td> <td>   -0.115</td> <td>    0.025</td>
</tr>
<tr>
  <th>RM</th>    <td>    5.0922</td> <td>    0.458</td> <td>   11.109</td> <td> 0.000</td> <td>    4.192</td> <td>    5.993</td>
</tr>
<tr>
  <th>LSTAT</th> <td>   -0.5651</td> <td>    0.057</td> <td>   -9.854</td> <td> 0.000</td> <td>   -0.678</td> <td>   -0.452</td>
</tr>
<tr>
  <th>B</th>     <td>    0.0090</td> <td>    0.003</td> <td>    2.952</td> <td> 0.003</td> <td>    0.003</td> <td>    0.015</td>
</tr>
<tr>
  <th>TAX</th>   <td>   -0.0060</td> <td>    0.002</td> <td>   -2.480</td> <td> 0.013</td> <td>   -0.011</td> <td>   -0.001</td>
</tr>
<tr>
  <th>AGE</th>   <td>    0.0236</td> <td>    0.014</td> <td>    1.653</td> <td> 0.099</td> <td>   -0.004</td> <td>    0.052</td>
</tr>
<tr>
  <th>ZN</th>    <td>    0.0294</td> <td>    0.013</td> <td>    2.198</td> <td> 0.028</td> <td>    0.003</td> <td>    0.056</td>
</tr>
<tr>
  <th>NOX</th>   <td>    3.4838</td> <td>    3.833</td> <td>    0.909</td> <td> 0.364</td> <td>   -4.047</td> <td>   11.014</td>
</tr>
<tr>
  <th>INDUS</th> <td>    0.0293</td> <td>    0.065</td> <td>    0.449</td> <td> 0.654</td> <td>   -0.099</td> <td>    0.157</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>195.490</td> <th>  Durbin-Watson:     </th> <td>   0.848</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 872.873</td> 
</tr>
<tr>
  <th>Skew:</th>          <td> 1.686</td>  <th>  Prob(JB):          </th> <td>2.87e-190</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 8.479</td>  <th>  Cond. No.          </th> <td>1.04e+04</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.04e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems.




```python
# 세변수만 추가한 모델의 회귀 계수 
fitted_multi_model.params
```




    const   -2.562251
    CRIM    -0.102941
    RM       5.216955
    LSTAT   -0.578486
    dtype: float64




```python
# full모델의 회귀계수
fitted_multi_model2.params
```




    const   -7.108827
    CRIM    -0.045293
    RM       5.092238
    LSTAT   -0.565133
    B        0.008974
    TAX     -0.006025
    AGE      0.023619
    ZN       0.029377
    NOX      3.483832
    INDUS    0.029270
    dtype: float64




```python
# base모델과 full모델의 잔차비교 
import matplotlib.pyplot as plt
fitted_multi_model.resid.plot(label="full")
fitted_multi_model2.resid.plot(label="full_add")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f955b10f220>




    
![png](output_17_1.png)
    


# 상관계수/산점도를 통해 다중공선성 확인


```python
# 상관행렬 
x_data2.corr()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>RM</th>
      <th>LSTAT</th>
      <th>B</th>
      <th>TAX</th>
      <th>AGE</th>
      <th>ZN</th>
      <th>NOX</th>
      <th>INDUS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>CRIM</th>
      <td>1.000000</td>
      <td>-0.219247</td>
      <td>0.455621</td>
      <td>-0.385064</td>
      <td>0.582764</td>
      <td>0.352734</td>
      <td>-0.200469</td>
      <td>0.420972</td>
      <td>0.406583</td>
    </tr>
    <tr>
      <th>RM</th>
      <td>-0.219247</td>
      <td>1.000000</td>
      <td>-0.613808</td>
      <td>0.128069</td>
      <td>-0.292048</td>
      <td>-0.240265</td>
      <td>0.311991</td>
      <td>-0.302188</td>
      <td>-0.391676</td>
    </tr>
    <tr>
      <th>LSTAT</th>
      <td>0.455621</td>
      <td>-0.613808</td>
      <td>1.000000</td>
      <td>-0.366087</td>
      <td>0.543993</td>
      <td>0.602339</td>
      <td>-0.412995</td>
      <td>0.590879</td>
      <td>0.603800</td>
    </tr>
    <tr>
      <th>B</th>
      <td>-0.385064</td>
      <td>0.128069</td>
      <td>-0.366087</td>
      <td>1.000000</td>
      <td>-0.441808</td>
      <td>-0.273534</td>
      <td>0.175520</td>
      <td>-0.380051</td>
      <td>-0.356977</td>
    </tr>
    <tr>
      <th>TAX</th>
      <td>0.582764</td>
      <td>-0.292048</td>
      <td>0.543993</td>
      <td>-0.441808</td>
      <td>1.000000</td>
      <td>0.506456</td>
      <td>-0.314563</td>
      <td>0.668023</td>
      <td>0.720760</td>
    </tr>
    <tr>
      <th>AGE</th>
      <td>0.352734</td>
      <td>-0.240265</td>
      <td>0.602339</td>
      <td>-0.273534</td>
      <td>0.506456</td>
      <td>1.000000</td>
      <td>-0.569537</td>
      <td>0.731470</td>
      <td>0.644779</td>
    </tr>
    <tr>
      <th>ZN</th>
      <td>-0.200469</td>
      <td>0.311991</td>
      <td>-0.412995</td>
      <td>0.175520</td>
      <td>-0.314563</td>
      <td>-0.569537</td>
      <td>1.000000</td>
      <td>-0.516604</td>
      <td>-0.533828</td>
    </tr>
    <tr>
      <th>NOX</th>
      <td>0.420972</td>
      <td>-0.302188</td>
      <td>0.590879</td>
      <td>-0.380051</td>
      <td>0.668023</td>
      <td>0.731470</td>
      <td>-0.516604</td>
      <td>1.000000</td>
      <td>0.763651</td>
    </tr>
    <tr>
      <th>INDUS</th>
      <td>0.406583</td>
      <td>-0.391676</td>
      <td>0.603800</td>
      <td>-0.356977</td>
      <td>0.720760</td>
      <td>0.644779</td>
      <td>-0.533828</td>
      <td>0.763651</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
## 상관행렬 시각화 해서 보기 
import seaborn as sns;
cmap = sns.light_palette("darkgray", as_cmap=True)
sns.heatmap(x_data2.corr(), annot=True, cmap=cmap)
plt.show()
```


    
![png](output_20_0.png)
    



```python
## 변수별 산점도 시각화
sns.pairplot(x_data2)
plt.show()
```


    
![png](output_21_0.png)
    


# VIF를 통한 다중공선성 확인 


```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(
    x_data2.values, i) for i in range(x_data2.shape[1])]
vif["features"] = x_data2.columns
##vif
vif
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>VIF Factor</th>
      <th>features</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.917332</td>
      <td>CRIM</td>
    </tr>
    <tr>
      <th>1</th>
      <td>46.535369</td>
      <td>RM</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8.844137</td>
      <td>LSTAT</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.856737</td>
      <td>B</td>
    </tr>
    <tr>
      <th>4</th>
      <td>19.923044</td>
      <td>TAX</td>
    </tr>
    <tr>
      <th>5</th>
      <td>18.457503</td>
      <td>AGE</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2.086502</td>
      <td>ZN</td>
    </tr>
    <tr>
      <th>7</th>
      <td>72.439753</td>
      <td>NOX</td>
    </tr>
    <tr>
      <th>8</th>
      <td>12.642137</td>
      <td>INDUS</td>
    </tr>
  </tbody>
</table>
</div>




```python
## nox 변수 제거후(X_data3) VIF 확인 

vif = pd.DataFrame()
x_data3= x_data2.drop('NOX',axis=1)
vif["VIF Factor"] = [variance_inflation_factor(
    x_data3.values, i) for i in range(x_data3.shape[1])]
vif["features"] = x_data3.columns
vif
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>VIF Factor</th>
      <th>features</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.916648</td>
      <td>CRIM</td>
    </tr>
    <tr>
      <th>1</th>
      <td>30.806301</td>
      <td>RM</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8.171214</td>
      <td>LSTAT</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.735751</td>
      <td>B</td>
    </tr>
    <tr>
      <th>4</th>
      <td>18.727105</td>
      <td>TAX</td>
    </tr>
    <tr>
      <th>5</th>
      <td>16.339792</td>
      <td>AGE</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2.074500</td>
      <td>ZN</td>
    </tr>
    <tr>
      <th>7</th>
      <td>11.217461</td>
      <td>INDUS</td>
    </tr>
  </tbody>
</table>
</div>




```python
## RM 변수 제거후(x_data4) VIF 확인 

vif = pd.DataFrame()
x_data4= x_data3.drop('RM',axis=1)
vif["VIF Factor"] = [variance_inflation_factor(
    x_data4.values, i) for i in range(x_data4.shape[1])]
vif["features"] = x_data4.columns
vif
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>VIF Factor</th>
      <th>features</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.907517</td>
      <td>CRIM</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.933529</td>
      <td>LSTAT</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.442569</td>
      <td>B</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.233237</td>
      <td>TAX</td>
    </tr>
    <tr>
      <th>4</th>
      <td>13.765377</td>
      <td>AGE</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1.820070</td>
      <td>ZN</td>
    </tr>
    <tr>
      <th>6</th>
      <td>11.116823</td>
      <td>INDUS</td>
    </tr>
  </tbody>
</table>
</div>




```python
# nox 변수 제거한 데이터(x_data3) 상수항 추가 후 회귀 모델 적합
# nox, rm 변수 제거한 데이터(x_data4) 상수항 추가 후 회귀 모델 적합
x_data3_=sm.add_constant(x_data3,has_constant="add")
x_data4_=sm.add_constant(x_data4,has_constant="add")
model_vif=sm.OLS(target,x_data3_)
fitted_model_vif=model_vif.fit()
model_vif2=sm.OLS(target,x_data4_)
fitted_model_vif2=model_vif2.fit()
```


```python
## 회귀모델 결과 비교 
fitted_model_vif.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>Target</td>      <th>  R-squared:         </th> <td>   0.662</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.656</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   121.6</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 27 Jul 2021</td> <th>  Prob (F-statistic):</th> <td>7.62e-112</td>
</tr>
<tr>
  <th>Time:</th>                 <td>22:58:00</td>     <th>  Log-Likelihood:    </th> <td> -1566.0</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3150.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   497</td>      <th>  BIC:               </th> <td>   3188.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   -5.9162</td> <td>    3.596</td> <td>   -1.645</td> <td> 0.101</td> <td>  -12.981</td> <td>    1.149</td>
</tr>
<tr>
  <th>CRIM</th>  <td>   -0.0451</td> <td>    0.036</td> <td>   -1.264</td> <td> 0.207</td> <td>   -0.115</td> <td>    0.025</td>
</tr>
<tr>
  <th>RM</th>    <td>    5.1027</td> <td>    0.458</td> <td>   11.138</td> <td> 0.000</td> <td>    4.203</td> <td>    6.003</td>
</tr>
<tr>
  <th>LSTAT</th> <td>   -0.5628</td> <td>    0.057</td> <td>   -9.825</td> <td> 0.000</td> <td>   -0.675</td> <td>   -0.450</td>
</tr>
<tr>
  <th>B</th>     <td>    0.0087</td> <td>    0.003</td> <td>    2.880</td> <td> 0.004</td> <td>    0.003</td> <td>    0.015</td>
</tr>
<tr>
  <th>TAX</th>   <td>   -0.0056</td> <td>    0.002</td> <td>   -2.344</td> <td> 0.019</td> <td>   -0.010</td> <td>   -0.001</td>
</tr>
<tr>
  <th>AGE</th>   <td>    0.0287</td> <td>    0.013</td> <td>    2.179</td> <td> 0.030</td> <td>    0.003</td> <td>    0.055</td>
</tr>
<tr>
  <th>ZN</th>    <td>    0.0284</td> <td>    0.013</td> <td>    2.130</td> <td> 0.034</td> <td>    0.002</td> <td>    0.055</td>
</tr>
<tr>
  <th>INDUS</th> <td>    0.0486</td> <td>    0.062</td> <td>    0.789</td> <td> 0.431</td> <td>   -0.072</td> <td>    0.170</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>193.530</td> <th>  Durbin-Watson:     </th> <td>   0.849</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 843.773</td> 
</tr>
<tr>
  <th>Skew:</th>          <td> 1.677</td>  <th>  Prob(JB):          </th> <td>5.98e-184</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 8.364</td>  <th>  Cond. No.          </th> <td>8.44e+03</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.44e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.




```python
fitted_model_vif2.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>Target</td>      <th>  R-squared:         </th> <td>   0.577</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.571</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   97.20</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 27 Jul 2021</td> <th>  Prob (F-statistic):</th> <td>5.53e-89</td>
</tr>
<tr>
  <th>Time:</th>                 <td>23:00:01</td>     <th>  Log-Likelihood:    </th> <td> -1622.3</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3261.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   498</td>      <th>  BIC:               </th> <td>   3294.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   29.6634</td> <td>    1.844</td> <td>   16.087</td> <td> 0.000</td> <td>   26.041</td> <td>   33.286</td>
</tr>
<tr>
  <th>CRIM</th>  <td>   -0.0329</td> <td>    0.040</td> <td>   -0.825</td> <td> 0.410</td> <td>   -0.111</td> <td>    0.045</td>
</tr>
<tr>
  <th>LSTAT</th> <td>   -0.9256</td> <td>    0.053</td> <td>  -17.589</td> <td> 0.000</td> <td>   -1.029</td> <td>   -0.822</td>
</tr>
<tr>
  <th>B</th>     <td>    0.0046</td> <td>    0.003</td> <td>    1.384</td> <td> 0.167</td> <td>   -0.002</td> <td>    0.011</td>
</tr>
<tr>
  <th>TAX</th>   <td>   -0.0048</td> <td>    0.003</td> <td>   -1.814</td> <td> 0.070</td> <td>   -0.010</td> <td>    0.000</td>
</tr>
<tr>
  <th>AGE</th>   <td>    0.0703</td> <td>    0.014</td> <td>    4.993</td> <td> 0.000</td> <td>    0.043</td> <td>    0.098</td>
</tr>
<tr>
  <th>ZN</th>    <td>    0.0513</td> <td>    0.015</td> <td>    3.490</td> <td> 0.001</td> <td>    0.022</td> <td>    0.080</td>
</tr>
<tr>
  <th>INDUS</th> <td>   -0.0357</td> <td>    0.068</td> <td>   -0.523</td> <td> 0.601</td> <td>   -0.170</td> <td>    0.098</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>138.742</td> <th>  Durbin-Watson:     </th> <td>   0.960</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 316.077</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 1.427</td>  <th>  Prob(JB):          </th> <td>2.32e-69</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 5.617</td>  <th>  Cond. No.          </th> <td>3.85e+03</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.85e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.




```python

```


```python

```

# 학습 / 검증데이터 분할


```python
from sklearn.model_selection import train_test_split
X = x_data2_
y = target
train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.7, test_size=0.3,random_state = 1)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)
```

    (354, 10) (152, 10) (354, 1) (152, 1)



```python
# train_x 회귀모델 적합
train_x.head()
fit_1=sm.OLS(train_y,train_x)
fit_1=fit_1.fit()
```


```python
## 검등데이터 에대한 예측값과 true값 비교 
plt.plot(np.array(fit_1.predict(test_x)),label="pred")
plt.plot(np.array(test_y),label="true")
plt.legend()
plt.show()
```


    
![png](output_34_0.png)
    



```python
## x_data3와 x_data4 학습 검증데이터 분할
X = x_data3_
y = target
train_x2, test_x2, train_y2, test_y2 = train_test_split(X, y, train_size=0.7, test_size=0.3,random_state = 1)

```


```python
X = x_data4_
y = target
train_x3, test_x3, train_y3, test_y3 = train_test_split(X, y, train_size=0.7, test_size=0.3,random_state = 1)

```


```python
test_y.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>307</th>
      <td>28.2</td>
    </tr>
    <tr>
      <th>343</th>
      <td>23.9</td>
    </tr>
    <tr>
      <th>47</th>
      <td>16.6</td>
    </tr>
    <tr>
      <th>67</th>
      <td>22.0</td>
    </tr>
    <tr>
      <th>362</th>
      <td>20.8</td>
    </tr>
  </tbody>
</table>
</div>




```python
# x_data3/x_data4 회귀 모델 적합(fit2,fit3)
fit_2=sm.OLS(train_y2,train_x2)
fit_2=fit_2.fit()
fit_3=sm.OLS(train_y3,train_x3)
fit_3=fit_3.fit()
```


```python
## true값과 예측값 비교 
plt.plot(np.array(fit_2.predict(test_x2)),label="pred1")
plt.plot(np.array(fit_3.predict(test_x3)),label="pred2")
plt.plot(np.array(test_y2),label="true")
plt.legend()
plt.show()
```


    
![png](output_39_0.png)
    



```python
## full모델 추가해서 비교 
plt.plot(np.array(fit_1.predict(test_x)),label="pred")
plt.plot(np.array(fit_2.predict(test_x2)),label="pred_vif")
plt.plot(np.array(fit_2.predict(test_x2)),label="pred_vif2")
plt.plot(np.array(test_y2),label="true")
plt.legend()
plt.show()
```


    
![png](output_40_0.png)
    



```python
plt.plot(np.array(test_y2['Target']-fit_1.predict(test_x)),label="pred_full")
plt.plot(np.array(test_y2['Target']-fit_2.predict(test_x2)),label="pred_vif")
plt.plot(np.array(test_y2['Target']-fit_3.predict(test_x3)),label="pred_vif2")
plt.legend()
plt.show()
```


    
![png](output_41_0.png)
    


# MSE를 통한 검증데이터에 대한 성능비교 


```python
from sklearn.metrics import mean_squared_error
```


```python
mean_squared_error(y_true=test_y['Target'],y_pred=fit_1.predict(tes))
```
